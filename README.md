# Empathetic Transformer Chatbot

A custom-built Transformer-based dialogue system designed to generate empathetic responses. This project implements a sequence-to-sequence model from scratch using PyTorch, trained on the **EmpatheticDialogues** dataset. It features emotion-conditioned generation and a custom BPE tokenizer.

## üìå Features

* **Custom Transformer Architecture**: Implemented from scratch (Encoder-Decoder) using PyTorch `nn.Module` (not pre-built Hugging Face wrappers).
* **Emotion Conditioning**: The model accepts emotion tags (e.g., `<emo_sad>`, `<emo_joy>`) to condition the response generation.
* **Multitask Learning**: Includes an auxiliary emotion classification head to improve context understanding during training.
* **Custom Tokenization**: Uses **SentencePiece** to train a BPE (Byte Pair Encoding) tokenizer specifically for this corpus.
* **Advanced Decoding**: Supports Greedy, Beam Search, and Top-k/Top-p Sampling with repetition penalties.
* **Evaluation Suite**: Includes Perplexity calculation and integration with BLEU, ROUGE-L, and chrF metrics.

## üìÇ Dataset

The model is trained on the **EmpatheticDialogues** dataset (Facebook AI).

* **Input Format**: `<emo_TAG> <sep> Situation <sep> Customer_Text Agent:`
* **Target Format**: `Agent_Response`
* **Preprocessing**: Text normalization, unicode cleaning, and splitting into Train (80%), Validation (10%), and Test (10%).

## ‚öôÔ∏è Model Configuration

The notebook uses the following hyperparameters for the Transformer model:

| Hyperparameter      | Value    | Description                                       |
|---------------------|----------|---------------------------------------------------|
| **Embedding Dim**   | 512      | Size of token embeddings                           |
| **FeedForward Dim** | 2048     | Internal dim of FFN layers (4 * 512)              |
| **Attention Heads** | 2        | Number of heads in Multi-Head Attention           |
| **Layers**          | 2        | Encoder and Decoder layers                         |
| **Dropout**         | 0.3      | Regularization probability                         |
| **Vocab Size**      | 32,000   | SentencePiece vocabulary size                      |
| **Max Seq Len**     | 256      | Maximum token length for inputs                   |

## üöÄ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/yourusername/empathetic-chatbot.git
   cd empathetic-chatbot
   ```

2. Install dependencies:
   ```bash
   pip install torch pandas numpy sentencepiece scikit-learn tqdm matplotlib sacrebleu rouge-score
   ```

## üõ†Ô∏è Usage

### 1. Data Preparation & Tokenizer Training

The notebook automatically handles data loading. It trains a SentencePiece model (`spm_bpe.model`) on the training corpus to handle the specific vocabulary of the dataset, including special tokens like `<emo_devastated>` or `<emo_excited>`.

### 2. Training

The training loop utilizes a custom `AdamWLite` optimizer and a composite loss function:

* **Primary Loss**: CrossEntropyLoss (with Label Smoothing = 0.1).
* **Auxiliary Loss**: Emotion classification loss (weighted at 0.1) to ensure the encoder representations capture emotional context.

```python
# Pseudo-code for training invocation
run_training_loop(
    model=model,
    train_loader=train_loader,
    val_loader=val_loader,
    epochs=30,
    lr=3e-4
)
```

### 3. Inference (Decoding)

The notebook provides multiple decoding strategies to generate responses.

**Beam Search (High Quality):**

```python
response = beam_search_decode(
    model, sp, src_ids, 
    num_beams=5, 
    length_penalty=0.7, 
    no_repeat_ngram_size=3
)
```

**Sampling (Creative/Diverse):**

```python
response = sample_decode(
    model, sp, src_ids, 
    top_k=40, 
    top_p=0.9, 
    temperature=0.9
)
```

## üìä Performance & Analysis

The notebook includes analysis tools to check for model diversity and generic responses:

* **Phrase Frequency Analysis**: Checks how often the model defaults to safe phrases like *"I'm sorry to hear that"* (approx 0.4% in the dataset).
* **N-gram Analysis**: Visualizes the most common unigrams and bigrams generated by the model.
* **Metrics**:
  - **Perplexity (PPL)**
  - **BLEU**
  - **ROUGE-L**
  - **chrF**

## üì¶ Artifacts

After training, the notebook exports a ZIP file `empathetic_transformer_export.zip` containing:

1. `transformer_state_dict.pt` (Model weights)
2. `spm_bpe.model` (Tokenizer model)
3. `spm_bpe.vocab` (Vocabulary file)
4. `inference_config.txt` (Hyperparameters for loading)

## üìú License

This project is open-source. Please refer to the specific license of the EmpatheticDialogues dataset regarding commercial use.

---

*Note: This project is designed for educational purposes to understand Transformer architecture implementation and empathetic text generation.*
```

Feel free to customize any sections as needed or add additional information relevant to your project!
